# Awesome Calibration

> A curated list of awesome papers about calibration. If I missed any papers, feel free to open a PR to include them! And any feedback and contributions are welcome!

## Papers

- [On Calibration of Modern Neural Networks](https://arxiv.org/abs/1706.04599) *Chuan Guo et.al.* ICML 2017.

- [When Does Label Smoothing Help?](https://arxiv.org/pdf/1906.02629.pdf) *Rafael Müller et.al.* NeurIPS 2019.

- [Calibrating Structured Output Predictors for Natural Language Processing](https://aclanthology.org/2020.acl-main.188.pdf) *Abhyuday Jagannatha et.al.* ACL 2020.

- [Selective Question Answering under Domain Shift](https://aclanthology.org/2020.acl-main.503.pdf) *Amita Kamath et.al.* ACL 2020. 

- [On the Inference Calibration of Neural Machine Translation](https://aclanthology.org/2020.acl-main.278.pdf) *Shuo Wang et.al.* ACL 2020.

- [Calibrating Deep Neural Networks using Focal Loss](https://arxiv.org/abs/2002.09437) *Jishnu Mukhoti et.al.* NeurIPS 2020.

- [Calibration of Pre-trained Transformers](https://arxiv.org/abs/2003.07892) *Shrey Desai et.al.* EMNLP 2020.

- [Calibrated Language Model Fine-Tuning for In- and Out-of-Distribution Data](https://aclanthology.org/2020.emnlp-main.102.pdf) *Lingkai Kong et.al.* EMNLP 2020.

- [Revisiting the Calibration of Modern Neural Networks](https://arxiv.org/abs/2106.07998) *Matthias Minderer et.al.* NeurIPS 2021.

- [How Can We Know When Language Models Know? On the Calibration of Language Models for Question Answering](https://aclanthology.org/2021.tacl-1.57/) *Zhengbao Jiang et.al.* TACL 2021.

- [Training Verifiers to Solve Math Word Problems](https://arxiv.org/abs/2110.14168) *Karl Cobbe et.al.* Arxiv 2021.

- [Reducing Conversational Agents’ Overconfidence Through Linguistic Calibration](https://aclanthology.org/2022.tacl-1.50.pdf)] *Sabrina J. Mielke et.al.* TACL 2022.

- [Teaching Models to Express Their Uncertainty in Words](https://arxiv.org/abs/2205.14334) *Stephanie Lin et.al.* TMLR 2022. 

- [Quantifying Uncertainty in Foundation Models via Ensembles](https://openreview.net/forum?id=LpBlkATV24M) *Meiqi Sun et.al.* NeurIPS@RobustSeq 2022

- [Language Models (Mostly) Know What They Know](https://arxiv.org/abs/2207.05221) *Saurav Kadavath et.al.* Arxiv 2022.

- [Re-Examining Calibration: The Case of Question Answering](https://arxiv.org/abs/2205.12507) *Chenglei Si et.al.* EMNLP 2022.

- [Calibrating Factual Knowledge in Pretrained Language Models](https://arxiv.org/abs/2210.03329) *Qingxiu Dong et.al.* EMNLP 2022.

- [Calibrated Interpretation: Confidence Estimation in Semantic Parsing](https://aclanthology.org/2023.tacl-1.69.pdf) *Elias Stengel-Eskin et.al.* TACL 2023.

- [Prompting GPT-3 To Be Reliable](https://arxiv.org/abs/2210.09150) *Chenglei Si et.al.* ICLR 2023.

- [Semantic Uncertainty: Linguistic Invariances for Uncertainty Estimation in Natural Language Generation](https://arxiv.org/abs/2302.09664) *Lorenz Kuhn et.al.* ICLR 2023.

- [Knowledge Card: Filling LLMs' Knowledge Gaps with Plug-in Specialized Language Models](https://arxiv.org/abs/2305.09955) *Shangbin Feng et.al.* Arxiv 2023.

- [A Close Look into the Calibration of Pre-trained Language Models](https://arxiv.org/abs/2211.00151) *Yangyi Chen et.al.* ACL 2023.

- [Do Large Language Models Know What They Don't Know?](https://arxiv.org/abs/2305.18153) *Zhangyue Yin et.al.* ACL 2023.

- [Investigating the Factual Knowledge Boundary of Large Language Models with Retrieval Augmentation](https://arxiv.org/abs/2307.11019) *Ruiyang Ren et.al.* Arxiv 2023.

- [Investigating Uncertainty Calibration of Aligned Language Models under the Multiple-Choice Setting](https://arxiv.org/abs/2310.11732) *Guande He et.al.* Arxiv 2023.

- [Can ChatGPT Defend its Belief in Truth? Evaluating LLM Reasoning via Debate](https://aclanthology.org/2023.findings-emnlp.795.pdf) *Boshi Wang et.al.* EMNLP 2023.

- [Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores from Language Models Fine-Tuned with Human Feedback](https://arxiv.org/pdf/2305.14975.pdf) *Katherine Tian et.al.* EMNLP 2023.

- [Navigating the Grey Area: How Expressions of Uncertainty and Overconfidence Affect Language Models](https://arxiv.org/abs/2302.13439) *Kaitlyn Zhou et.al.* EMNLP 2023.

- [On Uncertainty Calibration and Selective Generation in Probabilistic Neural Summarization: A Benchmark Study](https://aclanthology.org/2023.findings-emnlp.197/) *Polina Zablotskaia et.al.* EMNLP 2023.

- [The Curious Case of Hallucinatory (Un)answerability: Finding Truths in the Hidden States of Over-Confident Large Language Models](https://aclanthology.org/2023.emnlp-main.220.pdf) *Aviv Slobodkin et.al.* EMNLP 2023.

- [The Internal State of an LLM Knows When It's Lying](https://arxiv.org/abs/2304.13734) *Amos Azaria et.al.* EMNLP 2023.

- [Trustworthy LLMs: a Survey and Guideline for Evaluating Large Language Models' Alignment](https://arxiv.org/abs/2308.05374) Arxiv 2023.

- [A Survey of Safety and Trustworthiness of Large Language Models through the Lens of Verification and Validation](https://arxiv.org/abs/2305.11391) *Xiaowei Huang et.al.* Arxiv 2023.

- [Aligning Large Multimodal Models with Factually Augmented RLHF](https://arxiv.org/abs/2309.14525) *Zhiqing Sun et.al.* Arxiv 2023.

- [Trusted Source Alignment in Large Language Models](https://arxiv.org/abs/2311.06697) *Vasilisa Bashlovkina et.al.* Arxiv 2023.

- [R-Tuning: Teaching Large Language Models to Refuse Unknown Questions](https://arxiv.org/abs/2311.09677) *Hanning Zhang et.al.* Arxiv 2023.

- [Alignment for Honesty](https://arxiv.org/abs/2312.07000) *Yuqing Yang et.al.* Arxiv 2023.

- [Batch Calibration: Rethinking Calibration for In-Context Learning and Prompt Engineering](https://arxiv.org/abs/2309.17249) *Han Zhou et.al.* ICLR 2024.

- [LitCab: Lightweight Language Model Calibration over Short- and Long-form Responses](https://arxiv.org/abs/2310.19208) *Xin Liu et.al.* ICLR 2024.

- [Large Language Models Cannot Self-Correct Reasoning Yet](https://arxiv.org/abs/2310.01798) *Jie Huang et.al.* ICLR 2024.

- [Calibrated Language Models Must Hallucinate](https://arxiv.org/abs/2311.14648) *Adam Tauman Kalai et.al.* STOC 2024.

- [When Do LLMs Need Retrieval Augmentation? Mitigating LLMs’ Overconfidence Helps Retrieval Augmentation](https://arxiv.org/pdf/2402.11457.pdf) *Shiyu Ni et.al.* Arxiv 2024.

- [Fact-and-Reflection (FaR) Improves Confidence Calibration of Large Language Models](https://arxiv.org/abs/2402.17124) *Xinran Zhao et.al.* Arxiv 2024.

  